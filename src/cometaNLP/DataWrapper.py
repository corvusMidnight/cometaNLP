{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script ##\n",
    "import emoji\n",
    "import lib2to3.pgen2\n",
    "from lib2to3.pgen2 import token\n",
    "import pandas as pd\n",
    "import textstat\n",
    "import os\n",
    "from os import stat\n",
    "import operator\n",
    "from operator import index\n",
    "import re\n",
    "import regex\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "from textstat.textstat import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "from nltk.tag import pos_tag\n",
    "import matplotlib.pyplot as plt\n",
    "from cometaNLP import TextAnalyzer\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#SUBCLASS | SUBCLASS |SUBCLASS | SUBCLASS |SUBCLASS | SUBCLASS |SUBCLASS | SUBCLASS |SUBCLASS | SUBCLASS |SUBCLASS | SUBCLASS |SUBCLASS | SUBCLASS |SUBCLASS | SUBCLASS |\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
    "\n",
    "class DataWrapper(TextAnalyzer):\n",
    "    \"\"\"A subclass of the TextAnlyzer class. It is meant to analyze text data inside a dataframe\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, language: str, **args):\n",
    "        super().__init__(language)\n",
    "        \n",
    "    def data_wrapper(self, file_type: str, file_path: str, data_frame = False):\n",
    "\n",
    "        \"\"\"A class method that applies a series of tranformations to csv and tsv files\n",
    "        and returns a dictionary\n",
    "\n",
    "        data_wrapper() reads .csv and .tsv file, applies the functions and methods within the\n",
    "        the TextAnlyzer module to the column contaning the comments/text in the dataframe\n",
    "        in meaningful order, and finally converts it into a dictionary by index according to the following format:\n",
    "        {index -> {column -> value}}. The dictionary contains relevant information\n",
    "        for each comment in the dataset.\n",
    "        To make the function as comprehensive as possible, the user is asked to enter\n",
    "        whether the csv/tsv file has a header or not and the index of the column\n",
    "        on which they wish to apply the transformations. The column should be the one containing the comments\\text\n",
    "        data that the user whishes to analyze.\n",
    "\n",
    "        Args:\n",
    "            self: reference to the current instance of the class\n",
    "            file_type (str): A string (csv/tsv)\n",
    "            file_path (str): A string containing a file path to a csv/tsv file\n",
    "            data_frame (bool): If set to true, the function returns additionally pandas DataFrame object\n",
    "                               rather than a dictionary\n",
    "\n",
    "        Returns:\n",
    "            output (dict): A nested dictionary {index -> {column -> value}} containing\n",
    "                           relevant data and metadata for each comment in the input dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                                \"num_terms\", \"num_words\", \"num_unique_words\"]\n",
    "        \n",
    "        df = self.load(file_type, file_path)\n",
    "        \n",
    "        #Identifying the column containing the hate comments\n",
    "        c_column = int(input(f'What is the index of the column containing the comments? Remember: Index starts from 0 in Python'))\n",
    "        comments = df.iloc[:, c_column]\n",
    "\n",
    "\n",
    "        #Applying the functions to extract data and metadata\n",
    "        df['n_hashtags'] =  df.iloc[:, c_column].apply(TextAnalyzer.count_hashtags)\n",
    "        df['n_urls'] =  df.iloc[:, c_column].apply(TextAnalyzer.count_url)\n",
    "        df['n_user_tags'] =  df.iloc[:, c_column].apply(TextAnalyzer.count_user_tags)\n",
    "\n",
    "        df['clean_comments'] = df.iloc[:, c_column].apply(TextAnalyzer.preprocessor)\n",
    "        df['clean_comments'] = df['clean_comments'].apply(TextAnalyzer.punctuation_removal)\n",
    "\n",
    "        df['n_emojis'] = df.iloc[:, c_column].apply(TextAnalyzer.count_emoji)\n",
    "        df['clean_comments'] = df['clean_comments'].apply(TextAnalyzer.demojizer)\n",
    "        \n",
    "        #Tokenizer choice depending on language\n",
    "        if self.language == 'italian':\n",
    "            df['tokenized_comments'] =  df['clean_comments'].apply(self.italian_tokenizer)\n",
    "        else:\n",
    "            df['tokenized_comments'] =  df['clean_comments'].apply(self.tokenizer)\n",
    "\n",
    "\n",
    "        df['length'] =  df['tokenized_comments'].apply(TextAnalyzer.comment_length)\n",
    "        df['TTR'] =  df['tokenized_comments'].apply(TextAnalyzer.type_token_ratio)\n",
    "        df['CFR'] =  df['tokenized_comments'].apply(self.content_function_ratio)\n",
    "\n",
    "\n",
    "        df['stop_words_removed'] =  df['tokenized_comments'].apply(self.stop_words_removal)\n",
    "        df['lemmatized_comments'] =  df['stop_words_removed'].apply(self.lemmatizer)\n",
    "\n",
    "        df['POS_comments'] =  df['lemmatized_comments'].apply(self.pos)\n",
    "\n",
    "\n",
    "        output = df.to_dict('index')\n",
    "\n",
    "        if data_frame == True:\n",
    "            return output, df\n",
    "        else:\n",
    "            return output\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def data_wrapper_summary(self, file_type, file_path, visualize = True) -> tuple:\n",
    "\n",
    "        \"\"\"A class method that returns the relevant data comparison based on grouping \n",
    "        comparison (e.g., X v. Y) rather than for each comment individually.\n",
    "\n",
    "        get_summary() is built upon the data_wrapper() method. If visualize is set to True, it also shows a simple visualization of all the\n",
    "        summarized data. It compares average number of emojis, hashtags,\n",
    "        urls, user tags, length, type-token ratio, content-function ratio for\n",
    "        two classes of comments.\n",
    "        Args:\n",
    "            self: reference to the current instance of the class\n",
    "            file_type (str): A string (csv/tsv)\n",
    "            file_path (str): A string containing a file path to a csv/tsv file\n",
    "\n",
    "        Returns:\n",
    "            tuple: a tuple of values.\n",
    "        \"\"\"\n",
    "        output, df = self.data_wrapper(file_type, file_path, data_frame=True)\n",
    "        l_column = int(input(f'''Enter the index of the categorical by which you want the data to be grouped by.\n",
    "                                 Remember: Index starts from 0 in Python'''))\n",
    "        \n",
    "        mean_emojis = df.groupby(df.iloc[:, l_column])['n_emojis'].mean()\n",
    "        mean_hash = df.groupby(df.iloc[:, l_column])['n_hashtags'].mean()\n",
    "        mean_urls = df.groupby(df.iloc[:, l_column])['n_urls'].mean()\n",
    "        mean_user_tags = df.groupby(df.iloc[:, l_column])['n_user_tags'].mean()\n",
    "        mean_length = df.groupby(df.iloc[:, l_column])['length'].mean()\n",
    "        mean_TTR = df.groupby(df.iloc[:, l_column])['TTR'].mean()\n",
    "        mean_CFR = df.groupby(df.iloc[:, l_column])['CFR'].mean()\n",
    "        \n",
    "        if visualize:\n",
    "            mean_emojis.plot(kind='bar')\n",
    "            plt.title('Mean number of emojis')\n",
    "            plt.show(),\n",
    "\n",
    "            mean_hash.plot(kind='bar')\n",
    "            plt.title('Mean number of emojis')\n",
    "            plt.show(),\n",
    "\n",
    "            mean_urls.plot(kind='bar')\n",
    "            plt.title('Mean number of urls')\n",
    "            plt.show(), \n",
    "\n",
    "            mean_user_tags.plot(kind='bar')\n",
    "            plt.title('Mean number of user tags')\n",
    "            plt.show(),\n",
    "\n",
    "            mean_length.plot(kind='bar')\n",
    "            plt.title('Mean length of the comments')\n",
    "            plt.show(),\n",
    "\n",
    "            mean_TTR.plot(kind='bar')\n",
    "            plt.title('Mean TTR')\n",
    "            plt.show(),\n",
    "\n",
    "            mean_CFR.plot(kind='bar')\n",
    "            plt.title('Mean CFR')\n",
    "            plt.show()\n",
    "\n",
    "        return mean_emojis, mean_hash, mean_urls, mean_user_tags, mean_length, mean_TTR, mean_CFR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58a51bcd190156487426da2ad09203c2d900446cfcac933d60f788d428342e49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
